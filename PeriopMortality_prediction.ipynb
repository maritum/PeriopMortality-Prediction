{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T02:49:22.642896Z",
     "start_time": "2022-05-02T02:49:22.632321Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import hashlib\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, fbeta_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T02:49:23.268312Z",
     "start_time": "2022-05-02T02:49:23.246572Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T02:49:23.730455Z",
     "start_time": "2022-05-02T02:49:23.715769Z"
    }
   },
   "outputs": [],
   "source": [
    "def y_binary(y_pred, threshold):\n",
    "    return (y_pred >= threshold).astype('int')\n",
    "\n",
    "\n",
    "def model_performance(y_test, y_pred_bi, y_pred, f_score, value_dict):\n",
    "\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred_bi),\n",
    "                            columns=['Pred0', 'Pred1'],\n",
    "                            index=['True0', 'True1'])\n",
    "    precision_val = np.round(\n",
    "        np.diagonal(conf_mat) / np.sum(conf_mat, axis=0) * 100, 2)\n",
    "    recall_val = np.round(\n",
    "        np.diagonal(conf_mat) / np.sum(conf_mat, axis=1) * 100, 2)\n",
    "    accuracy_val = np.round(\n",
    "        np.sum(np.diagonal(conf_mat)) / np.sum(conf_mat.values) * 100, 2)\n",
    "    auc_test = roc_auc_score(y_test, y_pred)\n",
    "    # Data to plot precision - recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    value_dict['f_score'].append(f_score)\n",
    "    value_dict['accuracy'].append(accuracy_val)\n",
    "    value_dict['recall'].append(recall_val[1])\n",
    "    value_dict['precision'].append(precision_val[1])  \n",
    "    print('CONFUSION MATRIX: \\n {}'.format(conf_mat))\n",
    "    print('___________________________________')\n",
    "    print('Precision: \\n {}'.format(precision_val))\n",
    "    print('___________________________________')\n",
    "    print('Recall: \\n {}'.format(recall_val))\n",
    "    print('___________________________________')\n",
    "    print('Accuracy: \\n {}'.format(accuracy_val))\n",
    "    print('ROC-AUC value: {}'.format(auc_test))\n",
    "    print('PR-AUC value: {}'.format(auc_precision_recall))\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    return value_dict\n",
    "\n",
    "def auc_performance(y_test, y_pred,  value_dict):\n",
    "\n",
    "    auc_test = roc_auc_score(y_test, y_pred)\n",
    "    # Data to plot precision - recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "\n",
    "    value_dict['roc-auc'].append(auc_test)\n",
    "    value_dict['pr-auc'].append(auc_precision_recall)\n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:27:59.590278Z",
     "start_time": "2022-04-29T20:27:59.494535Z"
    }
   },
   "outputs": [],
   "source": [
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sha256_hash(x):\n",
    "    m = hashlib.sha256()\n",
    "    m.update(str(x).encode('utf-8'))\n",
    "    return m.hexdigest().upper()\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    df_types = df.dtypes\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        # filter data types excluding object and datetime\n",
    "        if (col_type != object) and (col_type != 'M8[ns]'):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
    "                        np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
    "                        np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
    "                        np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(\n",
    "                        np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
    "                        np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) /\n",
    "                                        start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def y_binary(y_pred, threshold):\n",
    "    return (y_pred >= threshold).astype('int')\n",
    "\n",
    "\n",
    "def model_performance(y_test, y_pred_bi, y_pred):\n",
    "\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred_bi),\n",
    "                            columns=['Pred0', 'Pred1'],\n",
    "                            index=['True0', 'True1'])\n",
    "    precision_val = np.round(\n",
    "        np.diagonal(conf_mat) / np.sum(conf_mat, axis=0) * 100, 2)\n",
    "    recall_val = np.round(\n",
    "        np.diagonal(conf_mat) / np.sum(conf_mat, axis=1) * 100, 2)\n",
    "    accuracy_val = np.round(\n",
    "        np.sum(np.diagonal(conf_mat)) / np.sum(conf_mat.values) * 100, 2)\n",
    "    auc_test = roc_auc_score(y_test, y_pred)\n",
    "    # Data to plot precision - recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "\n",
    "    print('CONFUSION MATRIX: \\n {}'.format(conf_mat))\n",
    "    print('___________________________________')\n",
    "    print('Precision: \\n {}'.format(precision_val))\n",
    "    print('___________________________________')\n",
    "    print('Recall: \\n {}'.format(recall_val))\n",
    "    print('___________________________________')\n",
    "    print('Accuracy: \\n {}'.format(accuracy_val))\n",
    "    print('ROC-AUC value: {}'.format(auc_test))\n",
    "    print('PR-AUC value: {}'.format(auc_precision_recall))\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    \n",
    "def model_performance_dictionary(y_test, y_pred_bi, y_pred, f_score, value_dict):\n",
    "\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred_bi),\n",
    "                            columns=['Pred0', 'Pred1'],\n",
    "                            index=['True0', 'True1'])\n",
    "    \n",
    "    precision_val = np.round(\n",
    "        np.diagonal(conf_mat) / np.sum(conf_mat, axis=0) * 100, 2)\n",
    "    recall_val = np.round(\n",
    "        np.diagonal(conf_mat) / np.sum(conf_mat, axis=1) * 100, 2)\n",
    "    accuracy_val = np.round(\n",
    "        np.sum(np.diagonal(conf_mat)) / np.sum(conf_mat.values) * 100, 2)\n",
    "    specificity = np.round(conf_mat.iloc[0,0]/(conf_mat.iloc[0,1]+conf_mat.iloc[0,0])*100,2)\n",
    "    npv = np.round(conf_mat.iloc[0,0]/(conf_mat.iloc[1,0]+conf_mat.iloc[0,0])*100,2)\n",
    "    auc_test = roc_auc_score(y_test, y_pred)\n",
    "    # Data to plot precision - recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    value_dict['f_score'].append(f_score)\n",
    "    value_dict['accuracy'].append(accuracy_val)\n",
    "    value_dict['recall'].append(recall_val[1])\n",
    "    value_dict['precision'].append(precision_val[1])  \n",
    "\n",
    "    value_dict['specificity'].append(specificity)\n",
    "    value_dict['NPV'].append(npv)\n",
    "\n",
    "    return value_dict, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:28:35.733672Z",
     "start_time": "2022-04-29T20:28:35.654459Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\",\n",
    "                  text)  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T02:49:17.240259Z",
     "start_time": "2022-05-02T02:49:17.122869Z"
    }
   },
   "outputs": [],
   "source": [
    "main_new_dataset = '../data/train_final_5f_all_labs_ext_med_hash'\n",
    "\n",
    "# changing the exp_prefix directory below decides which features to load\n",
    "# and where to save output files\n",
    "\n",
    "# this variable is the column that we will use as the target variable for the model\n",
    "target = 'INPT_DEATH_YN'\n",
    "#target = 'AKIN_EVENT'\n",
    "\n",
    "df = load_obj(main_new_dataset)\n",
    "\n",
    "\n",
    "df.rename(columns={\n",
    "    \"OR_CASE_ID\": \"case_id\",\n",
    "    \"PAT_ID\": \"patientid\"\n",
    "},\n",
    "    inplace=True)\n",
    "\n",
    "df[\"case_id\"] = df[\"case_id\"].apply(get_sha256_hash)\n",
    "df = df.loc[:, ~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:03:07.061981Z",
     "start_time": "2022-04-17T03:03:07.058359Z"
    }
   },
   "outputs": [],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:03:08.087738Z",
     "start_time": "2022-04-17T03:03:08.080023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('feature_importance_final.txt')\n",
    "  \n",
    "# # returns JSON object as a dictionary\n",
    "boruta_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:03:09.993479Z",
     "start_time": "2022-04-17T03:03:09.883119Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[:,838:].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:28:59.212305Z",
     "start_time": "2022-04-29T20:28:58.627047Z"
    }
   },
   "outputs": [],
   "source": [
    "df['PRIM_SURG_PROV_ID'] = df['PRIM_SURG_PROV_ID'].replace('E1032',\n",
    "                                                          1032).astype(int)\n",
    "AKIN_THRESHOLD = 0\n",
    "df['AKIN_EVENT'] = df['AKI_AKIN_CLASS'].apply(lambda val: 1\n",
    "                                              if val > AKIN_THRESHOLD else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:29:14.866948Z",
     "start_time": "2022-04-29T20:29:14.819337Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.INPT_DEATH_YN.value_counts(normalize=True))\n",
    "\n",
    "print(df.AKIN_EVENT.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-16T22:55:29.166746Z",
     "start_time": "2022-04-16T22:55:24.095100Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf = df[(df[\"AGE_LT_90\"] >=18) & (df[\"AGE_LT_90\"] < 90)]\n",
    "newdf = newdf[~newdf['ADMSN_SURGERY_NUMBER_W_ANES'].isna()]\n",
    "newdf = newdf[~(newdf['ASA_STATUS']==6.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection for different models\n",
    "\n",
    "### Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T18:49:08.561705Z",
     "start_time": "2022-04-29T18:49:08.289545Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = df.iloc[:, np.r_[0:58,964]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T18:49:08.788962Z",
     "start_time": "2022-04-29T18:49:08.764757Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = [\n",
    "    'encounter_id', 'patientid', 'ADMSN_ID', 'ASA_STATUS', 'CASE_START',\n",
    "    'CASE_END', 'LAST_EF_RESULT_DATE', 'TOT_RBC', 'CRYSTALLOID_ML',\n",
    "    'COLLOID_ML', 'FLOOR_2_ICU_YN', 'POSTOP_AKI_AKIN_CLASS', 'AKI_AKIN_CLASS',\n",
    "    'AKIN_EVENT', 'INPT_DEATH_YN', 'PRIM_SURG_PROV_MINUTES', 'OPEN_ACCESS_YN',\n",
    "    'GYN_ONC_ERAS_YN','HCUP_CODE'\n",
    "]\n",
    "\n",
    "cat_feat = [\n",
    "    'case_id', 'SEX', 'LAST_EF', 'HCUP_DESC', 'PRIMARY_CPT', 'CPT_DESC',\n",
    "    'DATE_OF_SERVICE', 'CASE_SRV_NAME', 'CASE_SRV_NAME_GROUP','GROUP_AGE'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:11:55.941731Z",
     "start_time": "2022-04-29T19:11:55.929455Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = df[['DATE_OF_SERVICE', 'AKI_AKIN_CLASS', 'INPT_DEATH_YN']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:11:58.096591Z",
     "start_time": "2022-04-29T19:11:56.289374Z"
    }
   },
   "outputs": [],
   "source": [
    "df_labs = df.iloc[:, np.r_[58:835,964]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:11:58.316742Z",
     "start_time": "2022-04-29T19:11:58.099697Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = pd.concat([df_base, df_labs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:11:58.337702Z",
     "start_time": "2022-04-29T19:11:58.323819Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = ['AKI_AKIN_CLASS', 'AKIN_EVENT', 'INPT_DEATH_YN']\n",
    "\n",
    "cat_feat = ['DATE_OF_SERVICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:16:38.949163Z",
     "start_time": "2022-04-29T19:16:38.937794Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = df[['DATE_OF_SERVICE', 'AKI_AKIN_CLASS', 'INPT_DEATH_YN']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:16:39.433465Z",
     "start_time": "2022-04-29T19:16:39.397573Z"
    }
   },
   "outputs": [],
   "source": [
    "df_proc_name = df.iloc[:, np.r_[835:836,964]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:16:40.128682Z",
     "start_time": "2022-04-29T19:16:40.118569Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = pd.concat([df_base, df_proc_name], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:16:40.768123Z",
     "start_time": "2022-04-29T19:16:40.762627Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = ['AKI_AKIN_CLASS', 'AKIN_EVENT', 'INPT_DEATH_YN']\n",
    "\n",
    "cat_feat = ['DATE_OF_SERVICE', 'PROC_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:16:42.610784Z",
     "start_time": "2022-04-29T19:16:42.582916Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_model.PROC_NAME.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:09:30.236223Z",
     "start_time": "2022-04-29T20:09:30.222941Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = df[['DATE_OF_SERVICE', 'AKI_AKIN_CLASS', 'INPT_DEATH_YN']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:09:30.672792Z",
     "start_time": "2022-04-29T20:09:30.586640Z"
    }
   },
   "outputs": [],
   "source": [
    "df_med = df.iloc[:, 838:965].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:09:30.991800Z",
     "start_time": "2022-04-29T20:09:30.969560Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = pd.concat([df_base, df_med], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:09:31.567951Z",
     "start_time": "2022-04-29T20:09:31.559576Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = ['AKI_AKIN_CLASS', 'AKIN_EVENT', 'INPT_DEATH_YN']\n",
    "\n",
    "cat_feat = ['DATE_OF_SERVICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:29:45.636279Z",
     "start_time": "2022-04-29T20:29:43.976569Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = df.iloc[:, np.r_[0:58,576:834,964]].copy()\n",
    "\n",
    "num_feat = [\n",
    "    'encounter_id', 'patientid','ADMSN_ID', 'ASA_STATUS', 'CASE_START', 'CASE_END',\n",
    "    'LAST_EF_RESULT_DATE', 'TOT_RBC', 'CRYSTALLOID_ML', 'COLLOID_ML',\n",
    "    'FLOOR_2_ICU_YN', 'POSTOP_AKI_AKIN_CLASS', 'AKI_AKIN_CLASS', 'AKIN_EVENT',\n",
    "    'INPT_DEATH_YN', 'PRIM_SURG_PROV_MINUTES', 'OPEN_ACCESS_YN',\n",
    "    'GYN_ONC_ERAS_YN','HCUP_CODE'\n",
    "]\n",
    "\n",
    "cat_feat = [\n",
    "    'case_id', 'SEX', 'LAST_EF', 'HCUP_DESC', 'PRIMARY_CPT', 'CPT_DESC',\n",
    "    'DATE_OF_SERVICE', 'CASE_SRV_NAME', 'GROUP_AGE'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:22:37.971910Z",
     "start_time": "2022-04-29T19:22:37.883741Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = df.iloc[:, np.r_[0:58, 835:837,964]].copy()\n",
    "\n",
    "num_feat = [\n",
    "    'encounter_id', 'patientid','ADMSN_ID', 'ASA_STATUS', 'CASE_START', 'CASE_END',\n",
    "    'LAST_EF_RESULT_DATE', 'TOT_RBC', 'CRYSTALLOID_ML', 'COLLOID_ML',\n",
    "    'FLOOR_2_ICU_YN', 'POSTOP_AKI_AKIN_CLASS', 'AKI_AKIN_CLASS', 'AKIN_EVENT',\n",
    "    'INPT_DEATH_YN', 'PRIM_SURG_PROV_MINUTES', 'OPEN_ACCESS_YN',\n",
    "    'GYN_ONC_ERAS_YN'\n",
    "]\n",
    "\n",
    "cat_feat = [\n",
    "    'case_id', 'SEX', 'LAST_EF', 'HCUP_DESC', 'PRIMARY_CPT', 'CPT_DESC',\n",
    "    'DATE_OF_SERVICE', 'CASE_SRV_NAME', 'GROUP_AGE', 'PROC_NAME'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T02:53:23.483829Z",
     "start_time": "2022-04-17T02:53:23.341148Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = df.iloc[:, np.r_[0:58, 837:965]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T02:53:23.897272Z",
     "start_time": "2022-04-17T02:53:23.876043Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = [\n",
    "    'encounter_id','patientid', 'ADMSN_ID', 'ASA_STATUS', 'CASE_START', 'CASE_END',\n",
    "    'LAST_EF_RESULT_DATE', 'TOT_RBC', 'CRYSTALLOID_ML', 'COLLOID_ML',\n",
    "    'FLOOR_2_ICU_YN', 'POSTOP_AKI_AKIN_CLASS', 'AKI_AKIN_CLASS', 'AKIN_EVENT',\n",
    "    'INPT_DEATH_YN', 'PRIM_SURG_PROV_MINUTES', 'OPEN_ACCESS_YN',\n",
    "    'GYN_ONC_ERAS_YN'\n",
    "]\n",
    "\n",
    "cat_feat = [\n",
    "    'case_id', 'SEX', 'LAST_EF', 'HCUP_DESC', 'PRIMARY_CPT', 'CPT_DESC',\n",
    "    'DATE_OF_SERVICE', 'CASE_SRV_NAME', 'GROUP_AGE'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8&9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:29:36.205895Z",
     "start_time": "2022-04-29T19:29:35.817074Z"
    }
   },
   "outputs": [],
   "source": [
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:29:36.546090Z",
     "start_time": "2022-04-29T19:29:36.537227Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = [\n",
    "    'encounter_id', 'patientid','ADMSN_ID', 'ASA_STATUS', 'CASE_START', 'CASE_END',\n",
    "    'LAST_EF_RESULT_DATE', 'TOT_RBC', 'CRYSTALLOID_ML', 'COLLOID_ML',\n",
    "    'FLOOR_2_ICU_YN', 'POSTOP_AKI_AKIN_CLASS', 'AKI_AKIN_CLASS', 'AKIN_EVENT',\n",
    "    'INPT_DEATH_YN', 'PRIM_SURG_PROV_MINUTES', 'OPEN_ACCESS_YN',\n",
    "    'GYN_ONC_ERAS_YN', 'or_case_id'\n",
    "]\n",
    "\n",
    "cat_feat = [\n",
    "    'case_id', 'SEX', 'LAST_EF', 'HCUP_DESC', 'PRIMARY_CPT', 'CPT_DESC',\n",
    "    'DATE_OF_SERVICE', 'CASE_SRV_NAME', 'GROUP_AGE', 'PROC_NAME'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:29:37.171268Z",
     "start_time": "2022-04-29T19:29:37.152653Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.DATE_OF_SERVICE.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:29:59.889528Z",
     "start_time": "2022-04-29T20:29:57.313969Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_model[df_model['DATE_OF_SERVICE'] < '2019-01-01']\n",
    "df_test = df_model[df_model['DATE_OF_SERVICE'] >= '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:30:09.241045Z",
     "start_time": "2022-04-29T20:30:09.196120Z"
    }
   },
   "outputs": [],
   "source": [
    "patientid_test = set(df_test['patientid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:30:14.229277Z",
     "start_time": "2022-04-29T20:30:11.818161Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train['patientid'].isin(patientid_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:30:18.860886Z",
     "start_time": "2022-04-29T20:30:14.989174Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['ADMSN_ID'].dropna(inplace=True)\n",
    "df_test['ADMSN_ID'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_describe = df_model.describe()\n",
    "ci = []\n",
    "for i in df_describe:\n",
    "    tmp = df_describe[(df_mean[i] != inf_val)&(df_describe[i] != inf_val_neg)]\n",
    "    interval = st.t.interval(0.95, len(tmp[i])-1, \n",
    "                  loc=np.nanmean(tmp[i]), scale=st.sem(tmp[i],nan_policy='omit'))\n",
    "    round_ci = [round(num,3) for num in list(interval)]\n",
    "    ci.append(round_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:30:28.242529Z",
     "start_time": "2022-04-29T20:30:27.985106Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = df_model.select_dtypes(include='number').drop(\n",
    "    labels = num_feat, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:30:36.437285Z",
     "start_time": "2022-04-29T20:30:36.397014Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_features = df_model.select_dtypes(include=['object', 'category']).drop(\n",
    "    labels = cat_feat, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:30:53.090551Z",
     "start_time": "2022-04-29T20:30:53.069254Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train\n",
    "y_train = df_train['INPT_DEATH_YN'].values\n",
    "#y_train = df_train['AKIN_EVENT'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:19:20.667320Z",
     "start_time": "2022-04-29T20:19:18.585459Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Fit One Hot Encoder using default spase matrix\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(X_train[cat_features])\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print('Processing time', elapsed, 'seconds')\n",
    "\n",
    "X_train_sparse = encoder.transform(X_train[cat_features])\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print('Processing time', elapsed, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:19:21.661701Z",
     "start_time": "2022-04-29T20:19:21.648878Z"
    }
   },
   "outputs": [],
   "source": [
    "X_sparse = pd.DataFrame(X_train_sparse,\n",
    "                          columns=encoder.get_feature_names(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:43.575714Z",
     "start_time": "2022-04-29T19:30:21.320615Z"
    }
   },
   "outputs": [],
   "source": [
    "# # #Uncomment for word embeddings (Models 3,6,8,9)\n",
    "# custom_stopwords = set(\n",
    "#     stopwords.words(\"english\") + [\"than\", \"to\", \"and\", \"or\", \"of\"])\n",
    "\n",
    "# data = X_train.copy()\n",
    "\n",
    "# data[\"tokens\"] = data[\"PROC_NAME\"].map(\n",
    "#     lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# # Remove empty values and keep relevant columns\n",
    "# data = data.loc[data.tokens.map(lambda x: len(x) > 0), [\"PROC_NAME\", \"tokens\"]]\n",
    "\n",
    "# print('Original dataframe: {}'.format(X_train.shape))\n",
    "# print('Pre-processed dataframe: {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:43.895652Z",
     "start_time": "2022-04-29T19:30:43.582939Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment for word embeddings (Models 3,6,8,9)\n",
    "# docs = data[\"PROC_NAME\"].values\n",
    "# tokenized_docs = data['tokens'].values\n",
    "# vocab = Counter()\n",
    "# for token in tokenized_docs:\n",
    "#     vocab.update(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:19:36.204181Z",
     "start_time": "2022-04-29T20:19:35.197720Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment for word embeddings (Models 3,6,8,9)\n",
    "# # #model_word2vec = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=5, seed=42)\n",
    "# model_word2vec = load_obj('../word2vec_model3_new_100')\n",
    "# vectorized_docs = vectorize(tokenized_docs, model=model_word2vec)\n",
    "\n",
    "# X_train = X_train.join(\n",
    "#     pd.DataFrame(vectorized_docs,\n",
    "#                  columns=['proccode_{}'.format(col) for col in range(0, 100)]))\n",
    "\n",
    "# proc_name = ['proccode_{}'.format(col) for col in range(0, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:58.634372Z",
     "start_time": "2022-04-29T19:30:55.361129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment for word embeddings (Models 3,6,8,9)\n",
    "X_train_enc = np.hstack((X_train[numeric_features], X_train_sparse))\n",
    "# X_train_enc = np.hstack((X_train_enc, X_train[proc_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:58.655645Z",
     "start_time": "2022-04-29T19:30:58.637679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment for word embeddings (Models 3,6,8,9)\n",
    "feature_names = list(numeric_features.astype(str)) + list(\n",
    "    encoder.get_feature_names(cat_features)) #+ list(proc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:58.675245Z",
     "start_time": "2022-04-29T19:30:58.661737Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(X_train_enc, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:16.124505Z",
     "start_time": "2022-04-29T19:30:08.844Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment for feature selection (Model 9)\n",
    "# feature_names = boruta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:17:34.191450Z",
     "start_time": "2022-04-29T19:17:34.144322Z"
    }
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "feature_names = [\n",
    "    regex.sub(\"_\", col) if any(x in str(col)\n",
    "                               for x in set(('[', ']', '<'))) else col\n",
    "    for col in feature_names\n",
    "]\n",
    "X_train_enc = pd.DataFrame(X_train_enc[feature_names],\n",
    "                     columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T19:30:16.127040Z",
     "start_time": "2022-04-29T19:30:12.244Z"
    }
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=csr_matrix(X_train_enc),\n",
    "                     feature_names=feature_names,\n",
    "                     label=y_train,\n",
    "                     nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:15:44.952482Z",
     "start_time": "2022-04-17T03:15:44.946371Z"
    }
   },
   "outputs": [],
   "source": [
    "best_param = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'aucpr',\n",
    "    'seed': 1,\n",
    "    'verbosity': 1,\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 5,\n",
    "    'eta': 0.1,\n",
    "    'gamma': 0.7,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1,\n",
    "    'nthread': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:24:27.790035Z",
     "start_time": "2022-04-17T03:15:44.959292Z"
    }
   },
   "outputs": [],
   "source": [
    "model = xgb.train(params=best_param, dtrain=dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-29T20:20:05.660685Z",
     "start_time": "2022-04-29T20:20:05.613913Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test\n",
    "X_test['GENDER'].replace('U', 'M', inplace=True)\n",
    "X_test['ANES_TYPE_HANDOFF'].replace('L&amp;D NITROUS', np.nan, inplace=True)\n",
    "y_test = df_test['INPT_DEATH_YN'].values\n",
    "#y_test = df_test['AKIN_EVENT'].values\n",
    "\n",
    "X_test_sparse = encoder.transform(X_test[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:24:43.136465Z",
     "start_time": "2022-04-17T03:24:43.126036Z"
    }
   },
   "outputs": [],
   "source": [
    "save_obj(model,'model/xgboost_model1_mort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:30:02.866678Z",
     "start_time": "2022-04-17T03:30:02.665572Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test\n",
    "X_test['GENDER'].replace('U', 'M', inplace=True)\n",
    "X_test['ANES_TYPE_HANDOFF'].replace('L&amp;D NITROUS', np.nan, inplace=True)\n",
    "y_test = df_test['INPT_DEATH_YN'].values\n",
    "#y_test = df_test['AKIN_EVENT'].values\n",
    "\n",
    "X_test_sparse = encoder.transform(X_test[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:30:05.709834Z",
     "start_time": "2022-04-17T03:30:05.075257Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc = np.hstack((X_test[numeric_features], X_test_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:30:12.273698Z",
     "start_time": "2022-04-17T03:30:06.043668Z"
    }
   },
   "outputs": [],
   "source": [
    "# # # Uncomment for word embeddings (Models 3,6,8,9)\n",
    "# data = X_test.copy()\n",
    "\n",
    "# data[\"tokens\"] = data[\"PROC_NAME\"].map(\n",
    "#     lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# # Remove empty values and keep relevant columns\n",
    "# data = data.loc[data.tokens.map(lambda x: len(x) > 0), [\"PROC_NAME\", \"tokens\"]]\n",
    "\n",
    "# docs = data[\"PROC_NAME\"].values\n",
    "# tokenized_docs = data['tokens'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:30:15.671729Z",
     "start_time": "2022-04-17T03:30:12.277140Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment for word embeddings (Models 3,6,8,9)\n",
    "# vectorized_docs_test = vectorize(tokenized_docs, model=model_word2vec)\n",
    "# X_test = X_test.join(\n",
    "#     pd.DataFrame(vectorized_docs_test,\n",
    "#                  columns=['proccode_{}'.format(col) for col in range(0, 100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:30:16.017572Z",
     "start_time": "2022-04-17T03:30:15.676781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment for word embeddings (Models 3,6,8,9)\n",
    "feature_names = list(numeric_features.astype(str)) + \\\n",
    "    list(encoder.get_feature_names(cat_features))#+list(proc_name)\n",
    "#X_test_enc = np.hstack((X_test_enc, X_test[proc_name]))\n",
    "X_test_enc = pd.DataFrame(X_test_enc, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for feature selection (Model 9) + comment 2nd dtest initialization\n",
    "#feature_names= boruta_features\n",
    "#dtest = xgb.DMatrix(data=csr_matrix(X_test_enc[feature_names]),feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-17T03:31:35.018802Z",
     "start_time": "2022-04-17T03:31:34.800348Z"
    }
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(data=csr_matrix(X_test_enc),\n",
    "                     feature_names=feature_names)\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f1 = {'f_score':[],'accuracy':[],'recall':[],'precision':[],'specificity':[],'NPV':[]}\n",
    "results_f2 = {'f_score':[],'accuracy':[],'recall':[],'precision':[],'specificity':[],'NPV':[]}\n",
    "results_f3 = {'f_score':[],'accuracy':[],'recall':[],'precision':[],'specificity':[],'NPV':[]}\n",
    "n =1000\n",
    "trials = 50\n",
    "# cache for performance\n",
    "unique_pat_ids = X_test[\"patientid\"].unique()\n",
    "auc_dict = {'roc-auc':[],\n",
    "            'pr-auc':[]}\n",
    "\n",
    "for trial in tqdm(range(trials)):\n",
    "    #tmp = np.random.choice(X_test.index,size=n)\n",
    "    #sample = X_test[X_test.index.isin(tmp)].copy()#X_test.iloc[tmp,:].copy()\n",
    "    tmp = np.random.choice(unique_pat_ids,size=n)\n",
    "    sample = X_test.loc[X_test.apply(lambda x: x.patientid in tmp, axis=1)].copy()\n",
    "    y_test = sample['INPT_DEATH_YN'].values\n",
    "    #y_test = sample['AKIN_EVENT'].values\n",
    "    \n",
    "    data = sample.copy()\n",
    "\n",
    "#     data[\"tokens\"] = data[\"PROC_NAME\"].map(\n",
    "#         lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "#     # Remove empty values and keep relevant columns\n",
    "#     data = data.loc[data.tokens.map(lambda x: len(x) > 0), [\"PROC_NAME\", \"tokens\"]]\n",
    "\n",
    "#     docs = data[\"PROC_NAME\"].values\n",
    "#     tokenized_docs = data['tokens'].values\n",
    "#     vectorized_docs_test = vectorize(tokenized_docs, model=model_word2vec)\n",
    "#     sample = sample.join(\n",
    "#         pd.DataFrame(vectorized_docs_test,\n",
    "#                      columns=['proccode_{}'.format(col) for col in range(0, 100)]))\n",
    "    \n",
    "\n",
    "    X_test_sparse = encoder.transform(sample[cat_features],)\n",
    "    X_test_enc = np.hstack((sample[numeric_features], X_test_sparse))\n",
    "    \n",
    "    feature_names = list(numeric_features.astype(str)) + \\\n",
    "    list(encoder.get_feature_names(cat_features))#+list(proc_name)\n",
    "    #X_test_enc = np.hstack((X_test_enc, sample[proc_name]))\n",
    "    X_test_enc = pd.DataFrame(X_test_enc, columns=feature_names)\n",
    "    \n",
    "    #feature_names= boruta_features\n",
    "    #dtest = xgb.DMatrix(data=csr_matrix(X_test_enc[feature_names]),feature_names = feature_names)\n",
    "\n",
    "    \n",
    "    dtest = xgb.DMatrix(data=csr_matrix(X_test_enc),\n",
    "                     feature_names=feature_names)\n",
    "    y_pred = model6.predict(dtest)\n",
    "    \n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "    \n",
    "    scores = [f1_score(y_test, y_binary(y_pred, t)) for t in thresholds]\n",
    "    scores_beta_1 = [\n",
    "        fbeta_score(y_test, y_binary(y_pred, t), beta=2.5)\n",
    "        for t in thresholds\n",
    "    ]\n",
    "    scores_beta_2 = [\n",
    "        fbeta_score(y_test, y_binary(y_pred, t), beta=3.0)\n",
    "        for t in thresholds\n",
    "    ]\n",
    "\n",
    "    ix_f1 = np.argmax(scores)\n",
    "    ix_fbeta_1 = np.argmax(scores_beta_1)\n",
    "    ix_fbeta_2 = np.argmax(scores_beta_2)\n",
    "    #print('Model performance with the optimal threshold for F1')\n",
    "    y_pred_binary_f1 = [1 if x >= thresholds[ix_f1] else 0 for x in y_pred]\n",
    "    results_f1,cm1 = model_performance(y_test, y_pred_binary_f1, y_pred,scores[ix_f1],results_f1)\n",
    "    #print('______________________________')\n",
    "    #print('Model performance with the optimal threshold for Fbeta (beta=2.0)')\n",
    "    y_pred_binary_f2 = [1 if x >= thresholds[ix_fbeta_1] else 0 for x in y_pred]\n",
    "    results_f2,cm2 = model_performance(y_test, y_pred_binary_f2, y_pred, scores_beta_1[ix_fbeta_1], results_f2)\n",
    "    #print('______________________________')\n",
    "    #print('Model performance with the optimal threshold for Fbeta (beta=3.0)')\n",
    "    y_pred_binary_f3 = [1 if x >= thresholds[ix_fbeta_2] else 0 for x in y_pred]\n",
    "    results_f3,cm3 = model_performance(y_test, y_pred_binary_f3, y_pred,scores_beta_2[ix_fbeta_2], results_f3)\n",
    "    #auc_dict = auc_performance(y_test,y_pred, auc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_enc_1 = pd.DataFrame(X_train_enc, columns=feature_names)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer.shap_values(X_train_enc)\n",
    "shap.summary_plot(shap_values,\n",
    "                  X_test_enc_1,\n",
    "                  feature_names=feature_names,\n",
    "                  plot_type=\"bar\",\n",
    "                  max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(model,'../models/main/xgboost_model_tune')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
